{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week3\n",
    "\n",
    "- Sequence to Sequence Model\n",
    "- Beam Search\n",
    "- Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq\n",
    "\n",
    "seq2seq 是一个Encoder–Decoder 结构的网络，它的输入是一个序列，输出也是一个序列， Encoder 中将一个可变长度的信号序列变为固定长度的向量表达，Decoder 将这个固定长度的向量变成可变长度的目标的信号序列。\n",
    "\n",
    "<img src=\"src/pictures/c5_w3_2.jpeg\">\n",
    "\n",
    "RNN中当前时间步的隐藏状态由上一个时间步的隐藏状态和输入决定\n",
    "$$h_t = f(h_{t-1}, x_t)$$\n",
    "获得各个时间步的隐藏层, 将隐藏层信息进行汇总生成语义向量\n",
    "$$C=q(h_1, h_2, h_3,...,h_{T_x})$$\n",
    "\n",
    "最简单的语义向量就是$T_x$时间步的隐藏层作为语义向量.\n",
    "\n",
    "解码阶段, 根据语义C和之前已经生成的输出序列来预测下一时间步的输出单词$y_t$, 有\n",
    "$$y_t=argmax_y P(y_t) \\\\ \n",
    "\\qquad = argmax \\prod_{t=1}^T p(y_t|y_1, y_2, \\dots, y_{t-1}, C) \\\\\n",
    "\\qquad = argmax \\sum_{t=1}^T log p(y_t|y_1, y_2, \\dots, y_{t-1}, C)$$\n",
    "\n",
    "对于机器翻译等问题, 输出的目标是获得最可能的句子, 即要求$P(y_1, y_2, \\dots, y_t|X)$整体最大, 如果使用Greedy Search, 在t时间步选择概率最大的单词$y_t$作为输出, 不能够得到整体最优, 因此提出了Beam Search(束搜索)来获得近似全局最优的句子.\n",
    "\n",
    "## Beam Search\n",
    "\n",
    "束搜索的直观理解是在t时间步选择概率最大的B个$\\{y_1, y_2, \\dots, y_t\\}$序列代替只选择概率当前概率最大的序列, 其中B称为束宽度. 如B=2的情形如下图:\n",
    "\n",
    "![B=2](src/pictures/c5_w3_4.png)\n",
    "\n",
    "基本的B结构偏向于短句, 因为连乘小于1的数值, 越长数值一定越小, 为解决这个问题, 提出长度正则化, 将序列的概率计算变为:\n",
    "$$\\frac{1}{T^\\alpha} argmax \\sum_{t=1}^T log p(y_t|y_1, y_2, \\dots, y_{t-1}, C)$$\n",
    "\n",
    "其中$\\alpha$控制正则化的柔和程度, 当$\\alpha=1$时最严苛, 当$\\alpha = 0$时没有正则化.\n",
    "\n",
    "B的参数选择也对搜索的性能具有影响, B越小,搜索越快搜索结果更差;B越大计算量更大搜索结果更好. 工业上常采用B=10, 20, 研究中最大常才用到1000.\n",
    "\n",
    "### 加入BS的误差分析\n",
    "\n",
    "如何判断使用BS机制和RNN结构进行训练时, 出现误差时是哪个部分出现了问题?\n",
    "\n",
    "可以对输入的目标结果(人工翻译的结果), 使用上述的RNN的权重和上述的评价式计算其概率$y^*$, 同时计算机器翻译的结果$\\hat{y}$, 若$y^* > \\hat{y}$, 说明BS不能够搜索到最优的结果;若$y^* \\leq \\hat{y}$, 说明RNN不能正确的估算正确结果的概率.\n",
    "\n",
    "## Attention\n",
    "\n",
    "Attention机制使神经网络在当前时间步只关注序列的一部分. 在Seq2Seq模型中加入Attention机制, 是利用Decoder上一个时间步t-1的隐藏层信息$S_{t-1}$和encoder的各时间步$t'$的输出$a_{t'}$通过函数g获得在encoder的各时间步的输出a对当前Decoder时间步t的影响的权重, 换句话说, 使用$S_{t-1}$作为查询在encoder输出$\\{a_1, a_2, \\dots, a_t\\}$为键构成的字典中查询, 获得字典值的加权输出.即\n",
    "$$\\alpha^{<t,t'>}=g(S_{t-1}, a_{t'}) \\\\\n",
    "Context_t = \\alpha_t \\cdot a $$\n",
    "其中$\\alpha_t = \\{\\alpha^{<t,1>}, \\alpha^{<t,2>}, \\dots, \\alpha^{<t,t'>}\\}$\n",
    "\n",
    "其中, g函数的实现有不同的版本, Andrew给出的是利用一个共享参数的全连接层处理,再接一个softmax层得到权重.\n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"src/pictures/c5_w3_5.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"src/pictures/c5_w3_6.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BaseAttention(nn.Module):\n",
    "    '''\n",
    "    Attention mechanism gets the output of encoder a(a_hidden_size, batch, str_len), \n",
    "    and hidden state s_t-1(s_hidden_size, batch) from the last time step in  decoder\n",
    "    '''\n",
    "    def __init__(self, a_dim, s_dim):\n",
    "        super(attention, self).__init__()\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(a_dim + s_dim, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.RelU())\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    \n",
    "    def forward(self, a, st):\n",
    "        '''\n",
    "        a -- (batch, str_len, a_dim)\n",
    "        st -- (batch, s_dim)\n",
    "        '''\n",
    "        str_len = a.shape[1]\n",
    "        s = st.unsqueeze(1).repeat(1, str_len, 1)\n",
    "        concat = torch.cat((a, s), axis=2) # (batch, str_len, a_dim+s_dim)\n",
    "        a = self.dense(concat) # (batch, str_len, 1)\n",
    "        alpha = self.softmax(a2) # (batch, str_len, 1)\n",
    "        context = alpha.mul(a) # (batch, str_len, a_dim)\n",
    "        return context\n",
    "\n",
    "def test():\n",
    "    a_dim, s_dim = 10, 8\n",
    "    att = BaseAttention(a_dim, s_dim)\n",
    "    a = torch.rand(3, 5, a_dim)\n",
    "    st = torch.rand(3, s_dim)\n",
    "    context = att(a, st)\n",
    "    print(context)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同机制"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
