# Deep Learning Specialization 学习笔记

Course 1 Deep Learning Specialization
-[ Week2](https://nbviewer.jupyter.org/github/caidwang/NN-and-DL-Notebook/blob/master/C1week2.ipynb)
    - logistic回归 -- 二分类器
        - sigmoid函数
        - loss函数 && MSE和Cross Entropy方法对比
        - cost函数
        - 前向传播与反向传播
    - numpy中的广播机制
    - numpy的二分类器实现
- [Week3](https://nbviewer.jupyter.org/github/caidwang/NN-and-DL-Notebook/blob/master/C1Week3.ipynb)
    - 浅层神经网络
        - 神经网络的表示
        - 正向传播和反向传播
        - 激活函数 && sigmoid和relu, tanh的比较
        - 梯度下降的学习方法
        - 随机初始化

Course 2 Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization
- [Week3](https://nbviewer.jupyter.org/github/caidwang/NN-and-DL-Notebook/blob/master/C2week3.ipynb) 
    - 超参调整
        - 调整顺序(重要性排列)
        - 调整方法
    - Batch Normalization
        - 计算
        - 机制有效性
        - BN在test过程的正确性
    - TensorFlow基础
        - tensorflow基本操作
        - 使用tensorFlow训练全连接多分类器
